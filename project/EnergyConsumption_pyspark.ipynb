{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2cdcdfc4-c8ec-4b2f-96ee-d4ea58057eca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/python'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72737c95-007b-40ad-8fc6-13762705e546",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/09/29 18:14:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init('/Users/subhasishbhaumik/Documents/Work/spark3')\n",
    "# Initializing the spark context\n",
    "import pyspark.pandas as ps\n",
    "#pdf_incidents = df_incidents.to_pandas_on_spark()\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Configure Spark to use multiple threads\n",
    "spark = SparkSession.builder.appName(\"Energy Consumption\")\\\n",
    "    .master(\"local[*]\")\\\n",
    "    .config(\"spark.executorEnv.PYSPARK_PYTHON\", \"/opt/anaconda3/bin/python\")\\\n",
    "    .config(\"spark.executor.instances\", \"4\")\\\n",
    "    .config(\"spark.executor.cores\", \"2\")\\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "257a8c2e-364c-456e-9847-5eb08583f923",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, col, rand, explode, sequence, to_date, datediff, expr, lit, when\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, TimestampType\n",
    "import random\n",
    "from datetime import datetime, timedelta\n",
    "import numpy as np\n",
    "from faker import Faker\n",
    "\n",
    "# Initialize Spark session\n",
    "#spark = SparkSession.builder.appName(\"EnergyConsumption\").getOrCreate()\n",
    "\n",
    "# import os\n",
    "# os.environ['PYSPARK_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "# os.environ['PYSPARK_DRIVER_PYTHON'] = '/opt/anaconda3/bin/python'\n",
    "\n",
    "# Initialize Faker\n",
    "fake = Faker()\n",
    "\n",
    "# Constants\n",
    "START_DATE = datetime(2020, 1, 1)\n",
    "END_DATE = datetime(2024, 12, 31)\n",
    "NUM_POWER_PLANTS = 50\n",
    "NUM_TRANSMISSION_LINES = 200\n",
    "NUM_SUBSTATIONS = 500\n",
    "NUM_DISTRIBUTION_NETWORKS = 1000\n",
    "NUM_CUSTOMERS = 1000000\n",
    "\n",
    "# Lists of major US cities and their approximate populations\n",
    "CITIES = [\n",
    "    (\"New York City\", 8336817), (\"Los Angeles\", 3898747), (\"Chicago\", 2746388),\n",
    "    (\"Houston\", 2304580), (\"Phoenix\", 1608139), (\"Philadelphia\", 1603797),\n",
    "    (\"San Antonio\", 1434625), (\"San Diego\", 1386932), (\"Dallas\", 1304379),\n",
    "    (\"San Jose\", 1013240), (\"Austin\", 961855), (\"Jacksonville\", 911507),\n",
    "    (\"Fort Worth\", 909585), (\"Columbus\", 898553), (\"San Francisco\", 873965),\n",
    "    (\"Charlotte\", 885708), (\"Indianapolis\", 876384), (\"Seattle\", 753675),\n",
    "    (\"Denver\", 727211), (\"Washington\", 689545), (\"Boston\", 675647),\n",
    "    (\"El Paso\", 681728), (\"Detroit\", 639111), (\"Nashville\", 689447),\n",
    "    (\"Portland\", 641162), (\"Memphis\", 633104), (\"Oklahoma City\", 649021),\n",
    "    (\"Las Vegas\", 651319), (\"Louisville\", 633045), (\"Baltimore\", 585708),\n",
    "    (\"Milwaukee\", 577222), (\"Albuquerque\", 564559), (\"Tucson\", 548073),\n",
    "    (\"Fresno\", 542107), (\"Sacramento\", 513624), (\"Mesa\", 504258),\n",
    "    (\"Kansas City\", 508090), (\"Atlanta\", 498715), (\"Long Beach\", 466742),\n",
    "    (\"Omaha\", 486051), (\"Raleigh\", 467665), (\"Colorado Springs\", 478221),\n",
    "    (\"Miami\", 442241), (\"Virginia Beach\", 459470), (\"Oakland\", 440646),\n",
    "    (\"Minneapolis\", 429606), (\"Tulsa\", 413066), (\"Arlington\", 398112),\n",
    "    (\"New Orleans\", 383997), (\"Wichita\", 389255)\n",
    "]\n",
    "\n",
    "# Events that might affect energy consumption\n",
    "EVENTS = [\n",
    "    (\"2020-03-15\", \"2020-06-30\", \"COVID-19 Lockdowns\", -0.2),\n",
    "    (\"2021-02-13\", \"2021-02-17\", \"Texas Winter Storm\", 0.5),\n",
    "    (\"2021-06-15\", \"2021-09-15\", \"Summer Heatwave\", 0.3),\n",
    "    (\"2022-06-01\", \"2022-08-31\", \"Energy Price Spike\", -0.1),\n",
    "    (\"2023-01-01\", \"2023-12-31\", \"Economic Recession\", -0.15),\n",
    "    (\"2024-06-01\", \"2024-08-31\", \"Olympic Games\", 0.2)\n",
    "]\n",
    "\n",
    "# UDFs\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"asset_id\", IntegerType(), True),\n",
    "    StructField(\"asset_type\", StringType(), True)\n",
    "]))\n",
    "def generate_asset(asset_id, asset_type):\n",
    "    return (asset_id, asset_type)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"plant_id\", IntegerType(), True),\n",
    "    StructField(\"plant_name\", StringType(), True),\n",
    "    StructField(\"capacity\", FloatType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"asset_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_power_plant(plant_id):\n",
    "    plant_types = [\"Coal\", \"Natural Gas\", \"Nuclear\", \"Hydroelectric\", \"Solar\", \"Wind\"]\n",
    "    plant_type = random.choice(plant_types)\n",
    "    capacity = random.uniform(100, 2000)  # MW\n",
    "    city, _ = random.choice(CITIES)\n",
    "    return (plant_id, f\"{city} {plant_type} Plant\", capacity, city, plant_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"line_id\", IntegerType(), True),\n",
    "    StructField(\"line_name\", StringType(), True),\n",
    "    StructField(\"voltage\", IntegerType(), True),\n",
    "    StructField(\"length\", FloatType(), True),\n",
    "    StructField(\"plant_id\", IntegerType(), True),\n",
    "    StructField(\"asset_id\", IntegerType(), True)\n",
    "]))\n",
    "# def generate_transmission_line(line_id, power_plants):\n",
    "#     voltages = [110, 220, 345, 500, 765]  # kV\n",
    "#     plant = random.choice(power_plants)\n",
    "#     return (line_id, f\"Line {line_id}\", random.choice(voltages), random.uniform(50, 500), plant[0], line_id)\n",
    "\n",
    "def generate_transmission_line(line_id):\n",
    "    voltages = [110, 220, 345, 500, 765]  # kV\n",
    "    plant = random.choice(power_plants)\n",
    "    return (line_id, f\"Line {line_id}\", random.choice(voltages), random.uniform(50, 500),10, line_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"substation_id\", IntegerType(), True),\n",
    "    StructField(\"substation_name\", StringType(), True),\n",
    "    StructField(\"capacity\", FloatType(), True),\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"asset_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_substation(substation_id):\n",
    "    city, _ = random.choice(CITIES)\n",
    "    return (substation_id, f\"{city} Substation {substation_id}\", random.uniform(100, 1000), city, substation_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"network_id\", IntegerType(), True),\n",
    "    StructField(\"network_name\", StringType(), True),\n",
    "    StructField(\"voltage\", FloatType(), True),\n",
    "    StructField(\"substation_id\", IntegerType(), True),\n",
    "    StructField(\"asset_id\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "# def generate_distribution_network(network_id, substations):\n",
    "#     substation = random.choice(substations)\n",
    "#     return (network_id, f\"Network {network_id}\", 11.0, substation[0], network_id)\n",
    "\n",
    "def generate_distribution_network(network_id):\n",
    "    substation = random.choice(substations)\n",
    "    return (network_id, f\"Network {network_id}\", 11.0, substation[0], network_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"network_id\", IntegerType(), True)\n",
    "]))\n",
    "\n",
    "\n",
    "def generate_customer(customer_id):\n",
    "    network = random.choice(networks)\n",
    "    city = next(sub for sub in substations if sub[0] == network[3])[3]\n",
    "    #print(city)\n",
    "    addr= random.choice(adrs.filter(adrs.city==city).select([lit(fake.street_address()).alias('street'),'zip','lat','lng','state_name',lit('US').alias('country')]).collect())                    \n",
    "    #print(addr)\n",
    "    return(customer_id, fake.name(),f\"{addr[0]} ,[{addr[2]},{addr[3]}],{addr[4]},{addr[1]}, {addr[5]}\",network[0])\n",
    "\n",
    "\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"meter_id\", IntegerType(), True),\n",
    "    StructField(\"meter_type\", StringType(), True),\n",
    "    StructField(\"installation_date\", DateType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_meter(meter_id, customer_id):\n",
    "    meter_types = [\"Smart\", \"Analog\", \"Digital\"]\n",
    "    return (meter_id, random.choice(meter_types), fake.date_between(start_date=START_DATE, end_date=END_DATE), customer_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"consumption_id\", IntegerType(), True),\n",
    "    StructField(\"meter_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"consumption\", FloatType(), True)\n",
    "]))\n",
    "def generate_consumption(consumption_id, meter_id, date, base_consumption):\n",
    "    # Strong seasonal variation\n",
    "    month = date.month\n",
    "    day_of_year = date.timetuple().tm_yday\n",
    "    # print(month)\n",
    "    # print(day_of_year)\n",
    "    \n",
    "    # Summer peak (July) and winter peak (January) with smoother transitions\n",
    "    seasonal_factor = 1 + 0.5 * (np.sin((day_of_year - 15) * 2 * np.pi / 365) + \n",
    "                                 0.5 * np.sin((day_of_year - 15) * 4 * np.pi / 365))\n",
    "    \n",
    "    # Temperature variation (approximate, you may want to use actual temperature data for more accuracy)\n",
    "    temp_variation = random.gauss(0, 0.1)  # Random normal distribution\n",
    "    seasonal_factor += temp_variation\n",
    "    \n",
    "    # Weekly pattern (higher consumption on weekdays)\n",
    "    weekday_factor = 1.1 if date.weekday() < 5 else 0.9\n",
    "    \n",
    "    # Apply random daily variation\n",
    "    daily_variation = random.uniform(0.9, 1.1)\n",
    "    \n",
    "    # Apply event effects\n",
    "    event_effect = 1\n",
    "    for event_start, event_end, _, effect in EVENTS:\n",
    "        if datetime.strptime(event_start, \"%Y-%m-%d\").date() <= date <= datetime.strptime(event_end, \"%Y-%m-%d\").date():\n",
    "            event_effect += effect\n",
    "\n",
    "    \n",
    "    # Calculate final consumption\n",
    "    consumption = base_consumption * seasonal_factor * weekday_factor * daily_variation * event_effect\n",
    "    return (consumption_id, meter_id, date, max(0, consumption))\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"bill_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"amount\", FloatType(), True),\n",
    "    StructField(\"consumption_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_billing(bill_id, customer_id, consumption_id, consumption, date):\n",
    "    rate = random.uniform(0.1, 0.2)  # $/kWh\n",
    "    amount = consumption * rate\n",
    "    return (bill_id, customer_id, date, amount, consumption_id)\n",
    "\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"outage_id\", IntegerType(), True),\n",
    "    StructField(\"start_time\", TimestampType(), True),\n",
    "    StructField(\"end_time\", TimestampType(), True),\n",
    "    StructField(\"description\", StringType(), True),\n",
    "    StructField(\"asset_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_outage(outage_id):\n",
    "    asset = random.choice(assets)\n",
    "    start_time = fake.date_time_between(start_date=START_DATE, end_date=END_DATE)\n",
    "    end_time = start_time + timedelta(hours=random.randint(1, 24))\n",
    "    return (outage_id, start_time, end_time, f\"Outage on {asset[1]}\", asset[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e5427af8-769c-40a6-b982-f2a8717fd1e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data\n",
    "assets_df = spark.range(1, NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + NUM_SUBSTATIONS + NUM_DISTRIBUTION_NETWORKS + 1) \\\n",
    "    .withColumn(\"asset_type\", when(col(\"id\") <= NUM_POWER_PLANTS, \"Power Plant\")\n",
    "                .when(col(\"id\") <= NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES, \"Transmission Line\")\n",
    "                .when(col(\"id\") <= NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + NUM_SUBSTATIONS, \"Substation\")\n",
    "                .otherwise(\"Distribution Network\")) \\\n",
    "    .withColumn(\"asset\", generate_asset(\"id\", \"asset_type\")) \\\n",
    "    .select(\"asset.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13efb78-0633-4480-a99a-30f5422fb65c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "power_plants_df = spark.range(1, NUM_POWER_PLANTS + 1) \\\n",
    "    .withColumn(\"plant\", generate_power_plant(\"id\")) \\\n",
    "    .select(\"plant.*\")\n",
    "power_plants=power_plants_df.collect()\n",
    "power_plants_df.createOrReplaceTempView(\"power_plants_df\")\n",
    "## Transmission Lines\n",
    "transmission_lines_df = spark.range(NUM_POWER_PLANTS + 1, NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + 1) \\\n",
    "    .withColumn(\"line\", generate_transmission_line(\"id\")) \\\n",
    "    .select(\"line.*\")\n",
    "transmission_lines_df.createOrReplaceTempView(\"transmission_lines_df\")\n",
    "substations_df = spark.range(NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + 1, NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + NUM_SUBSTATIONS + 1) \\\n",
    "    .withColumn(\"substation\", generate_substation(\"id\")) \\\n",
    "    .select(\"substation.*\")\n",
    "substations_df.createOrReplaceTempView(\"substations_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e207cd27-6710-4ca5-be67-cf8dadd08b41",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "substations=substations_df.collect()\n",
    "distribution_networks_df = spark.range(NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + NUM_SUBSTATIONS + 1, NUM_POWER_PLANTS + NUM_TRANSMISSION_LINES + NUM_SUBSTATIONS + NUM_DISTRIBUTION_NETWORKS + 1) \\\n",
    "    .withColumn(\"network\", generate_distribution_network(\"id\")) \\\n",
    "    .select(\"network.*\")\n",
    "distribution_networks_df.createOrReplaceTempView(\"distribution_networks_df\")\n",
    "#distribution_networks_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "213c82a7-189a-4c11-b539-87a262534bb8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nsql=\"\"\"\n",
    "select d.network_id , s.city from\n",
    "distribution_networks_df d \n",
    "inner join substations_df s\n",
    "on s.substation_id=d.substation_id\n",
    "\"\"\"\n",
    "network_city = spark.sql(nsql).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "101a7e05-b93c-40d4-b45c-3f94b02127c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "address = pd.read_csv('/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/uszips.csv')\n",
    "@udf(returnType=StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"customer_name\", StringType(), True),\n",
    "    StructField(\"address\", StringType(), True),\n",
    "    StructField(\"network_id\", IntegerType(), True)\n",
    "]))\n",
    "def generate_customer(customer_id):\n",
    "    network = random.choice(network_city)\n",
    "    #print(addr)\n",
    "    # Filter the DataFrame based on a column value\n",
    "    filtered_df = address[address['city'] == network[1]]\n",
    "    addr=random.choice(filtered_df[['city','zip','lat','lng','state_name']].values.tolist())\n",
    "    addr_col=f\"{fake.street_address()} ,{addr[0]},{addr[1]},[{addr[2]},{addr[3]}],{addr[4]},US\"\n",
    "    # Print the filtered DataFrame\n",
    "    addr_col\n",
    "    return (customer_id, fake.name(),addr_col,network[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "40bb0033-dce9-4db4-8d24-ab0e658cb677",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+--------------------------------------------------------------------------------------+----------+\n",
      "|customer_id|customer_name     |address                                                                               |network_id|\n",
      "+-----------+------------------+--------------------------------------------------------------------------------------+----------+\n",
      "|1          |Marie Perry       |781 Mclean Walk Suite 318 ,Las Vegas,89102,[36.1453,-115.18662],Nevada,US             |857       |\n",
      "|2          |Carrie Dean       |402 Hernandez Views Apt. 838 ,Oklahoma City,73169,[35.3815,-97.64538],Oklahoma,US     |1370      |\n",
      "|3          |Gina Hammond      |390 Good Road Suite 912 ,Washington,20018,[38.92596,-76.97281],District of Columbia,US|1060      |\n",
      "|4          |Rodney Anderson   |73438 Lisa Centers ,Philadelphia,19147,[39.93645,-75.15468],Pennsylvania,US           |1708      |\n",
      "|5          |Allison Madden    |689 Turner Common ,Boston,2133,[42.35838,-71.06383],Massachusetts,US                  |756       |\n",
      "|6          |Brenda Villegas   |222 Stephenson Corner Suite 570 ,New Orleans,70115,[29.92375,-90.10219],Louisiana,US  |877       |\n",
      "|7          |Craig Spencer     |82996 Medina Village Suite 844 ,Jacksonville,32203,[30.33702,-81.67129],Florida,US    |1296      |\n",
      "|8          |Robert Hawkins    |7843 Christopher Junctions ,Dallas,75210,[32.77141,-96.74618],Texas,US                |1040      |\n",
      "|9          |Charles Brooks    |725 Jason Valleys ,Los Angeles,90043,[33.98876,-118.33519],California,US              |1106      |\n",
      "|10         |Craig Fox         |779 Rodriguez Lake ,Mesa,85209,[33.37877,-111.63476],Arizona,US                       |1542      |\n",
      "|11         |Laura Larson      |78874 Dennis Shoal ,Boston,40107,[37.76952,-85.63798],Kentucky,US                     |1379      |\n",
      "|12         |James Flores      |2585 Wilson Cove Suite 995 ,Oklahoma City,73145,[35.42427,-97.41403],Oklahoma,US      |1082      |\n",
      "|13         |Tiffany Ruiz      |480 Harper Road ,San Jose,95138,[37.2462,-121.73287],California,US                    |841       |\n",
      "|14         |Jeremy Sanchez    |9555 Geoffrey Plaza Apt. 258 ,Seattle,98174,[47.60477,-122.33528],Washington,US       |799       |\n",
      "|15         |Christopher Larsen|8670 Zachary Fords Apt. 988 ,Arlington,76017,[32.66223,-97.16237],Texas,US            |982       |\n",
      "|16         |Brian Sanders     |0452 Bethany Glen Apt. 310 ,San Jose,95112,[37.34461,-121.88346],California,US        |1679      |\n",
      "|17         |Charles Douglas   |827 Thompson Centers Apt. 474 ,Wichita,67226,[37.76898,-97.21915],Kansas,US           |847       |\n",
      "|18         |Haley Murphy      |32925 Schroeder Alley Suite 492 ,Virginia Beach,23454,[36.83216,-76.02616],Virginia,US|1382      |\n",
      "|19         |Mary Mckenzie     |27759 William Burg ,Tucson,85701,[32.21702,-110.97138],Arizona,US                     |1324      |\n",
      "|20         |Grant Hall        |9452 Carrie Pine Suite 718 ,Raleigh,27697,[35.7788,-78.62297],North Carolina,US       |1283      |\n",
      "+-----------+------------------+--------------------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "networks=distribution_networks_df.collect()\n",
    "customers_df = spark.range(1, NUM_CUSTOMERS + 1) \\\n",
    "    .withColumn(\"customer\", generate_customer(\"id\")) \\\n",
    "    .select(\"customer.*\")\n",
    "customers_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "65ccbc2c-8f45-4493-9aa5-2cca5093b018",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/29 18:41:56 ERROR Executor: Exception in task 4.0 in stage 59.0 (TID 264)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/09/29 18:41:56 WARN TaskSetManager: Lost task 4.0 in stage 59.0 (TID 264) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "24/09/29 18:41:56 ERROR TaskSetManager: Task 4 in stage 59.0 failed 1 times; aborting job\n",
      "24/09/29 18:41:56 WARN TaskSetManager: Lost task 0.0 in stage 59.0 (TID 260) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 59.0 failed 1 times, most recent failure: Lost task 4.0 in stage 59.0 (TID 264) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/29 18:41:56 WARN TaskSetManager: Lost task 3.0 in stage 59.0 (TID 263) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 59.0 failed 1 times, most recent failure: Lost task 4.0 in stage 59.0 (TID 264) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[113], line 9\u001b[0m\n\u001b[1;32m      4\u001b[0m customers_df\u001b[38;5;241m.\u001b[39mcreateOrReplaceTempView(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcustomers_df\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m sql_cust\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124mselect * from customers_df where \u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124mlength(address) >=0\u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m spark\u001b[38;5;241m.\u001b[39msql(sql_cust)\u001b[38;5;241m.\u001b[39mcount()\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/pyspark/sql/dataframe.py:1238\u001b[0m, in \u001b[0;36mDataFrame.count\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m   1216\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns the number of rows in this :class:`DataFrame`.\u001b[39;00m\n\u001b[1;32m   1217\u001b[0m \n\u001b[1;32m   1218\u001b[0m \u001b[38;5;124;03m    .. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1236\u001b[0m \u001b[38;5;124;03m    3\u001b[39;00m\n\u001b[1;32m   1237\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1238\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jdf\u001b[38;5;241m.\u001b[39mcount())\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/29 18:41:56 WARN TaskSetManager: Lost task 7.0 in stage 59.0 (TID 267) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 4 in stage 59.0 failed 1 times, most recent failure: Lost task 4.0 in stage 59.0 (TID 264) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.hashAgg_doAggregateWithoutKey_0$(Unknown Source)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.shuffle.sort.BypassMergeSortShuffleWriter.write(BypassMergeSortShuffleWriter.java:140)\n",
      "\tat org.apache.spark.shuffle.ShuffleWriteProcessor.write(ShuffleWriteProcessor.scala:59)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:104)\n",
      "\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:54)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "#customers_df_n = customers_df.filter(length(col(address))>=0)\n",
    "\n",
    "customers_df.createOrReplaceTempView(\"customers_df\")\n",
    "sql_cust=\"\"\"\n",
    "select * from customers_df where \n",
    "length(address) >=0\n",
    "\"\"\"\n",
    "spark.sql(sql_cust).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9dbe4206-3537-4354-96a0-9c0a5b9cc6ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3a1d010f-a7ec-43e1-a933-d33d86f36a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_df = spark.range(1, NUM_CUSTOMERS + 1) \\\n",
    "    .withColumn(\"meter\", generate_meter(\"id\", \"id\")) \\\n",
    "    .select(\"meter.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "940ceff2-175a-45e5-8b89-17faa122a52c",
   "metadata": {},
   "outputs": [],
   "source": [
    "meters_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2ccf9-e94b-445d-b7f5-bb6c9f983f72",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "@udf(returnType=StructType([\n",
    "    StructField(\"consumption_id\", IntegerType(), True),\n",
    "    StructField(\"meter_id\", IntegerType(), True),\n",
    "    StructField(\"date\", DateType(), True),\n",
    "    StructField(\"consumption\", FloatType(), True)\n",
    "]))\n",
    "def generate_consumption(consumption_id, meter_id, date, base_consumption):\n",
    "    # Strong seasonal variation\n",
    "    month = date.month\n",
    "    day_of_year = date.timetuple().tm_yday\n",
    "    # print(month)\n",
    "    # print(day_of_year)\n",
    "    \n",
    "    # Summer peak (July) and winter peak (January) with smoother transitions\n",
    "    seasonal_factor = 1 + 0.5 * (np.sin((day_of_year - 15) * 2 * np.pi / 365) + \n",
    "                                 0.5 * np.sin((day_of_year - 15) * 4 * np.pi / 365))\n",
    "    \n",
    "    # Temperature variation (approximate, you may want to use actual temperature data for more accuracy)\n",
    "    temp_variation = random.gauss(0, 0.1)  # Random normal distribution\n",
    "    seasonal_factor += temp_variation\n",
    "    \n",
    "    # Weekly pattern (higher consumption on weekdays)\n",
    "    weekday_factor = 1.1 if date.weekday() < 5 else 0.9\n",
    "    \n",
    "    # Apply random daily variation\n",
    "    daily_variation = random.uniform(0.9, 1.1)\n",
    "    \n",
    "    # Apply event effects\n",
    "    event_effect = 1\n",
    "    for event_start, event_end, _, effect in EVENTS:\n",
    "        if datetime.strptime(event_start, \"%Y-%m-%d\").date() <= date <= datetime.strptime(event_end, \"%Y-%m-%d\").date():\n",
    "            event_effect += effect\n",
    "\n",
    "    \n",
    "    # Calculate final consumption\n",
    "    consumption = base_consumption * seasonal_factor * weekday_factor * daily_variation * event_effect\n",
    "    return (consumption_id, meter_id, date, max(0, consumption))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0eb815ad-259d-4892-9ac3-6acd2cb336ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import decimal\n",
    "def generate_consumption(consumption_id, meter_id, date, base_consumption):\n",
    "    # Strong seasonal variation\n",
    "    #date=datetime.strptime(date1, '%Y-%m-%d')\n",
    "    month = date.month\n",
    "    day_of_year = date.timetuple().tm_yday\n",
    "    # print(month)\n",
    "    # print(day_of_year)\n",
    "    \n",
    "    # Summer peak (July) and winter peak (January) with smoother transitions\n",
    "    seasonal_factor = 1 + 0.5 * (np.sin((day_of_year - 15) * 2 * np.pi / 365) + \n",
    "                                 0.5 * np.sin((day_of_year - 15) * 4 * np.pi / 365))\n",
    "    \n",
    "    # Temperature variation (approximate, you may want to use actual temperature data for more accuracy)\n",
    "    temp_variation = random.gauss(0, 0.1)  # Random normal distribution\n",
    "    seasonal_factor += temp_variation\n",
    "    \n",
    "    # Weekly pattern (higher consumption on weekdays)\n",
    "    weekday_factor = 1.1 if date.weekday() < 5 else 0.9\n",
    "    \n",
    "    # Apply random daily variation\n",
    "    daily_variation = random.uniform(0.9, 1.1)\n",
    "    \n",
    "    # Apply event effects\n",
    "    event_effect = 1\n",
    "    for event_start, event_end, _, effect in EVENTS:\n",
    "        # if datetime.strptime(event_start, \"%Y-%m-%d\").date() <= date <= datetime.strptime(event_end, \"%Y-%m-%d\").date():\n",
    "        #     event_effect += effect\n",
    "        if datetime.strptime(event_start, \"%Y-%m-%d\").date() <= date <= datetime.strptime(event_end, \"%Y-%m-%d\").date():\n",
    "            event_effect += effect\n",
    "\n",
    "    \n",
    "    # Calculate final consumption\n",
    "    consumption = base_consumption * seasonal_factor * weekday_factor * daily_variation * event_effect\n",
    "    return (consumption_id, meter_id, date, decimal.Decimal(round(max(0, consumption),2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c839b125-a4c7-451d-b8f5-057e40b6fac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_consumption(1,1,'2020-01-01',200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1108f7b2-89b4-40d1-9ef7-f2010ff110e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Generate consumption and billing data\n",
    "date_range = spark.sql(f\"SELECT explode(sequence(to_date('{START_DATE}'), to_date('{END_DATE}'), interval 1 day)) as date\")\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "\n",
    "consumption_df = meters_df.crossJoin(date_range) \\\n",
    "    .withColumn(\"base_consumption\", rand() * 800 + 200) \\\n",
    "    .withColumn(\"id\",monotonically_increasing_id())\\\n",
    "    .withColumn(\"consumption\", generate_consumption(\n",
    "        \"id\",\n",
    "        \"meter_id\",\n",
    "        \"date\",\n",
    "        col(\"base_consumption\")/30\n",
    "    )) \\\n",
    "    .select(\"consumption.*\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "af671721-da2c-4784-b7af-9de22b3cf854",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+-----------------+-----------+----------+------------------+---+------------------+\n",
      "|meter_id|meter_type|installation_date|customer_id|      date|  base_consumption| id|           con_inp|\n",
      "+--------+----------+-----------------+-----------+----------+------------------+---+------------------+\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-01|355.69333693815247|  0|11.856444564605082|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-02| 959.9835281652955|  1|31.999450938843186|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-03| 811.0975685634106|  2|27.036585618780354|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-04|246.17747043499287|  3| 8.205915681166429|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-05| 688.3761904070266|  4|22.945873013567553|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-06| 912.5896506989349|  5|30.419655023297828|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-07|  612.475552334773|  6|20.415851744492436|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-08| 782.7733599695553|  7| 26.09244533231851|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-09| 777.9696918725444|  8| 25.93232306241815|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-10|345.26790234726735|  9|11.508930078242246|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-11|496.33365307252495| 10|16.544455102417498|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-12| 297.7918301170157| 11| 9.926394337233857|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-13| 536.4073408944614| 12| 17.88024469648205|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-14| 889.7200277679572| 13|29.657334258931904|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-15| 801.6283176810396| 14| 26.72094392270132|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-16| 335.2505805334638| 15| 11.17501935111546|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-17|344.62389516508125| 16|11.487463172169376|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-18|260.22493433576017| 17| 8.674164477858673|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-19| 907.0901423163294| 18| 30.23633807721098|\n",
      "|       1|     Smart|       2024-11-03|          1|2020-01-20| 904.1196315664416| 19| 30.13732105221472|\n",
      "+--------+----------+-----------------+-----------+----------+------------------+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "date_range = spark.sql(f\"SELECT explode(sequence(to_date('{START_DATE}'), to_date('{END_DATE}'), interval 1 day)) as date\")\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "consumption_df = meters_df.crossJoin(date_range) \\\n",
    "    .withColumn(\"base_consumption\", rand() * 800 + 200) \\\n",
    "    .withColumn(\"id\",monotonically_increasing_id())\\\n",
    "    .withColumn(\"con_inp\",col(\"base_consumption\")/30)\n",
    "consumption_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0f68e54d-d19e-4777-908e-902cbabaf483",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType,FloatType,DecimalType\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"consumption_id\", IntegerType(), True),\n",
    "    StructField(\"meter_id\", IntegerType(), True),\n",
    "    StructField(\"consumption_date\", DateType(), True),\n",
    "    StructField(\"consumption\", DecimalType(), True)\n",
    "])\n",
    "_rdd=consumption_df.limit(1000000).rdd\n",
    "abc=_rdd.map(lambda row: generate_consumption(row[6],row[0],row[4],row[7]))\n",
    "consumption_df_1=spark.createDataFrame(abc,schema)\n",
    "consumption_df_1.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "71ef8f32-1a58-4b78-927d-61cfecb73943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- consumption_id: integer (nullable = true)\n",
      " |-- meter_id: integer (nullable = true)\n",
      " |-- consumption_date: date (nullable = true)\n",
      " |-- consumption: decimal(10,0) (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "consumption_df_1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "18c22152-fbc2-45bc-921f-6835d9bc8a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "billing_df = consumption_df_1 \\\n",
    "    .where(expr(\"day(consumption_date) = 1\")) \\\n",
    "    .groupBy(\"meter_id\", \"consumption_date\") \\\n",
    "    .agg({\"consumption\": \"sum\"}) \\\n",
    "    .withColumnRenamed(\"sum(consumption)\", \"total_consumption\") \\\n",
    "    .withColumn(\"billing\", generate_billing(\n",
    "        monotonically_increasing_id(),\n",
    "        \"meter_id\",\n",
    "        monotonically_increasing_id(),\n",
    "        \"total_consumption\",\n",
    "        \"consumption_date\"\n",
    "    )) \\\n",
    "    .select(\"billing.*\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "848833ec-7422-4880-8598-f1534e7e2e34",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "32841"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "billing_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8f7197d7-abd3-42fb-8663-8db5f54cbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "assets=assets_df.collect()\n",
    "outages_df = spark.range(1, 1001) \\\n",
    "    .withColumn(\"outage\", generate_outage(\"id\")) \\\n",
    "    .select(\"outage.*\")\n",
    "\n",
    "# Save DataFrames to CSV files\n",
    "def save_to_csv(df, filename):\n",
    "    df.write.csv(filename, header=True, mode=\"overwrite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "40c8ac73-9891-4d5e-b9ec-3f7cb7a37940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "save_to_csv(assets_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/assets')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "696255cd-99bc-41ea-85ab-e74c330490a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(power_plants_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/power_plants')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "dd1e818d-1ecd-4243-a40b-aa088c0f1e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(transmission_lines_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/transmission_lines')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "9beb1cff-e7cf-4eb2-b380-0bf7145659d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(substations_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/substations')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a206ba5d-d677-4ab7-b8ce-dd62d3b530b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(distribution_networks_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/distribution_networks')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "37bc4c77-1534-4634-868d-eee18d5d5a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/09/29 18:30:58 ERROR Executor: Exception in task 5.0 in stage 45.0 (TID 202)]\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/09/29 18:30:58 WARN TaskSetManager: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "24/09/29 18:30:58 ERROR TaskSetManager: Task 5 in stage 45.0 failed 1 times; aborting job\n",
      "24/09/29 18:30:58 ERROR FileFormatWriter: Aborting job e81bda7d-dee5-4612-a892-cb13ea105ba4.\n",
      "org.apache.spark.SparkException: Job aborted due to stage failure: Task 5 in stage 45.0 failed 1 times, most recent failure: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2398)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)\n",
      "\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)\n",
      "\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)\n",
      "\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)\n",
      "\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)\n",
      "\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)\n",
      "\tat org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:859)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:388)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:361)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:240)\n",
      "\tat org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:850)\n",
      "\tat java.base/jdk.internal.reflect.DirectMethodHandleAccessor.invoke(DirectMethodHandleAccessor.java:103)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:580)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\t... 1 more\n",
      "24/09/29 18:30:58 WARN TaskSetManager: Lost task 7.0 in stage 45.0 (TID 204) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 45.0 failed 1 times, most recent failure: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/29 18:30:58 WARN TaskSetManager: Lost task 4.0 in stage 45.0 (TID 201) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 45.0 failed 1 times, most recent failure: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/29 18:30:58 ERROR Utils: Aborting task\n",
      "org.apache.spark.TaskKilledException\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:597)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator$$anonfun$1.applyOrElse(PythonRunner.scala:594)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:38)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:99)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:91)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "24/09/29 18:30:58 WARN FileOutputCommitter: Could not delete file:/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/customers/_temporary/0/_temporary/attempt_202409291830578397541795440165813_0045_m_000000_197\n",
      "24/09/29 18:30:58 ERROR FileFormatWriter: Job job_202409291830578397541795440165813_0045 aborted.\n",
      "24/09/29 18:30:58 WARN TaskSetManager: Lost task 0.0 in stage 45.0 (TID 197) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 45.0 failed 1 times, most recent failure: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n",
      "24/09/29 18:30:58 WARN TaskSetManager: Lost task 3.0 in stage 45.0 (TID 200) (subonmacbookpro executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 5 in stage 45.0 failed 1 times, most recent failure: Lost task 5.0 in stage 45.0 (TID 202) (subonmacbookpro executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n",
      "  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n",
      "    raise IndexError('Cannot choose from an empty sequence')\n",
      "IndexError: Cannot choose from an empty sequence\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:94)\n",
      "\tat org.apache.spark.sql.execution.python.BasePythonUDFRunner$$anon$1.read(PythonUDFRunner.scala:75)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:491)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat scala.collection.Iterator$$anon$10.hasNext(Iterator.scala:460)\n",
      "\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage2.processNext(Unknown Source)\n",
      "\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n",
      "\tat org.apache.spark.sql.execution.WholeStageCodegenEvaluatorFactory$WholeStageCodegenPartitionEvaluator$$anon$1.hasNext(WholeStageCodegenEvaluatorFactory.scala:43)\n",
      "\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:385)\n",
      "\tat org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)\n",
      "\tat org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)\n",
      "\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)\n",
      "\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:331)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1570)\n",
      "\n",
      "Driver stacktrace:)\n"
     ]
    },
    {
     "ename": "PythonException",
     "evalue": "\n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPythonException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m save_to_csv(customers_df, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/customers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[55], line 8\u001b[0m, in \u001b[0;36msave_to_csv\u001b[0;34m(df, filename)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msave_to_csv\u001b[39m(df, filename):\n\u001b[0;32m----> 8\u001b[0m     df\u001b[38;5;241m.\u001b[39mwrite\u001b[38;5;241m.\u001b[39mcsv(filename, header\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverwrite\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/pyspark/sql/readwriter.py:1864\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[0;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[1;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode(mode)\n\u001b[1;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_opts(\n\u001b[1;32m   1847\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[1;32m   1848\u001b[0m     sep\u001b[38;5;241m=\u001b[39msep,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1862\u001b[0m     lineSep\u001b[38;5;241m=\u001b[39mlineSep,\n\u001b[1;32m   1863\u001b[0m )\n\u001b[0;32m-> 1864\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jwrite\u001b[38;5;241m.\u001b[39mcsv(path)\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1323\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/Documents/Work/spark3/python/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mPythonException\u001b[0m: \n  An exception was thrown from the Python worker. Please see the stack trace below.\nTraceback (most recent call last):\n  File \"/var/folders/9b/j4zsp5q576v916jdht_wj2s40000gn/T/ipykernel_60010/3665416466.py\", line 16, in generate_customer\n  File \"/opt/anaconda3/lib/python3.12/random.py\", line 347, in choice\n    raise IndexError('Cannot choose from an empty sequence')\nIndexError: Cannot choose from an empty sequence\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(customers_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/customers')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe31cd2-abb8-4377-8502-d08d4915fd2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv(meters_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/meters')\n",
    "save_to_csv(consumption_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/consumption')\n",
    "save_to_csv(billing_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/billing')\n",
    "save_to_csv(outages_df, '/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/ps/outages')\n",
    "\n",
    "print(\"Data generation complete. CSV files have been created.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b6a358c-b64e-4112-8739-ac58eeb2296b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e2bda0-73ff-4fbb-8063-792cf571f8d8",
   "metadata": {},
   "source": [
    "## Appendix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cbc5abd-28cc-4268-a289-2c995fd1f3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_address(city_name):\n",
    "    address = {\n",
    "        \"street_address\": fake.street_address(),\n",
    "        \"city\": city_name,\n",
    "        \"state\": fake.state(),\n",
    "        \"postal_code\": fake.postcode(),\n",
    "        \"country\": fake.country()\n",
    "    }\n",
    "    return address\n",
    "\n",
    "def generate_customer(customer_id):\n",
    "    network = random.choice(networks)\n",
    "    city = next(sub for sub in substations if sub[0] == network[3])[3]\n",
    "    print(city)\n",
    "    addr= random.choice(adrs.filter(adrs.city==city).select([lit(fake.street_address()).alias('street'),'zip','lat','lng','state_name',lit('US').alias('country')]).collect())                    \n",
    "    print(addr)\n",
    "    return(customer_id, fake.name(),f\"{addr[0]} ,[{addr[2]},{addr[3]}],{addr[4]},{addr[1]}, {addr[5]}\",network[0])\n",
    "    #return (customer_id, fake.name(), generate_fake_address(city).replace('\\n', ', ') , network[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "371b4e23-8cb1-49f9-bd78-0ba4055673ed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "generate_customer(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e129651-c3c6-405a-a179-9bcf3f3d9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fake_address(city_name):\n",
    "    Faker.seed(100)\n",
    "    return {\n",
    "        \"street_address\": fake.street_address(),\n",
    "        \"city\": city_name,\n",
    "        \"state\": fake.state(),\n",
    "        \"postal_code\": fake.zipcode(),\n",
    "        \"country\": fake.country()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a0ee82-b617-477e-bf51-689cc85a3042",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/uszips.csv\"\n",
    "adrs = spark.read.csv(path, header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd9006b-7db8-422c-9c01-b6d54e2ffc61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "adrs.filter(adrs.city=='Miami').select([lit(fake.street_address()),'city','zip','lat','lng','state_name',lit('US')]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d34b7-dafb-41fc-8963-dc42092aaae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4aea81-3414-4d74-89e5-09abb2d29503",
   "metadata": {},
   "outputs": [],
   "source": [
    "random.choices(CITIES, weights=[city[1] for city in CITIES])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63614d5-a1fc-4b2b-88af-f8bd17f71ded",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the CSV file into a pandas DataFrame\n",
    "address = pd.read_csv('/Users/subhasishbhaumik/Documents/neu/IE6750/project_data/uszips.csv')\n",
    "\n",
    "# Filter the DataFrame based on a column value\n",
    "filtered_df = address[address['city'] == 'Miami']\n",
    "addr=random.choice(filtered_df[['city','zip','lat','lng','state_name']].values.tolist())\n",
    "addr_col=f\"{fake.street_address()} ,{addr[0]},{addr[1]},[{addr[2]},{addr[3]}],{addr[4]},US\"\n",
    "# Print the filtered DataFrame\n",
    "addr_col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3b3ae-13f8-41f6-9d3b-16d0b9165f32",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "addr=random.choice(filtered_df[['city','zip','lat','lng','state_name']].values.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918e6a73-59f4-4b90-86e6-d13c4f35fb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"{fake.street_address()} ,{addr[0]},{addr[1]},[{addr[2]},{addr[3]}],{addr[4]},US\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d326333d-678d-43df-b259-130a3d108c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "addr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301afda9-3318-48a3-b260-1798786fb134",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9525c4df-c963-4fa6-b36d-8285b89744f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
